% add to Table of content
\addcontentsline{toc}{part}{ABSTRACT}

% set 0 indentation
\setlength{\parindent}{0pt}
% set paragraph space = 1 space
\setlength{\parskip}{1em}
% set line space = 1.5
\setlength{\baselineskip}{1.5em}

\begin{center}
  \fontsize{14}{17}\selectfont{\textbf{
    ABSTRACT
  }}
\end{center}
\vspace{2em}

\acrfull{vl} models have shown promising performance across multiple tasks in both zero-shot and fine-tuning setups. 
Most studies use \acrlong{mlm} as a pre-training task, applying random masking to image caption tokens. 
However, random token masking is not an optimal strategy for training \acrshort{vl} models, and effective masking strategies in \acrshort{vl} remain underexplored. 
In this work, we investigate the effects of \acrfull{pos} masking, as each \acrshort{pos} category contributes differently to sentence meaning. 
By pre-training models with different \acrshort{pos} masking strategies, we evaluate each model on image-text retrieval and visual question answering tasks, categorizing each question type following the VALSE.
Our findings contribute to a deeper understanding of how \acrshort{pos} masking influences model performance, providing insights that can lead to more effective pre-training strategies for future \acrshort{vl} models.

% 4)  key findings (2-3 sentences)
%     - summarize ONLY the key findings - it means interesting findings
% 5)  contributions
%     - why this is important to be solved; what impact it can bring
    
% Exercise: within 15 mins, write down these five sentences, and then put on the chat.
% \textbf{Keywords:} keyword1, keyword2. 