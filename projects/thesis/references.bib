%%% Text Encoder model
%BERT
@article{bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

%%% SEMI SUPERVISED LEARNING
% Meta Pseudo Label
@inproceedings{meta_pseudo,
    title={Meta pseudo labels},
    author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={11557--11568},
    year={2021}
}

% VIT Semi Supervised
@inproceedings{semisup-vit,
    title={Semi-supervised Vision Transformers at Scale},
    author={Zhaowei Cai and Avinash Ravichandran and Paolo Favaro and Manchen Wang and Davide Modolo and Rahul Bhotika and Zhuowen Tu and Stefano Soatto},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=7a2IgJ7V4W}
}

@article{aug_consistency,
    title={Unsupervised data augmentation for consistency training},
    author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={6256--6268},
    year={2020}
}

%Temperal Ensembling
@article{laine2016temporal,
    title={Temporal ensembling for semi-supervised learning},
    author={Laine, Samuli and Aila, Timo},
    journal={arXiv preprint arXiv:1610.02242},
    year={2016}
}

%EMA Teacher
@article{mean_teacher,
    title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
    author={Tarvainen, Antti and Valpola, Harri},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

%EMAN Teacher
@inproceedings{eman,
    title={Exponential moving average normalization for self-supervised and semi-supervised learning},
    author={Cai, Zhaowei and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Tu, Zhuowen and Soatto, Stefano},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={194--203},
    year={2021}
}

%FixMatch
@article{fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

%Interpolation consistency training
@article{inter_consistency,
    title={Interpolation consistency training for semi-supervised learning},
    author={Verma, Vikas and Kawaguchi, Kenji and Lamb, Alex and Kannala, Juho and Solin, Arno and Bengio, Yoshua and Lopez-Paz, David},
    journal={Neural Networks},
    volume={145},
    pages={90--106},
    year={2022},
    publisher={Elsevier}
}

%MixUp
@article{mixup,
    title={mixup: Beyond empirical risk minimization},
    author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
    journal={arXiv preprint arXiv:1710.09412},
    year={2017}
}

%MixMatch
@article{mixmatch,
    title={Mixmatch: A holistic approach to semi-supervised learning},
    author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

%Unsupervised Data Augmentation
@inproceedings{cubuk2020randaugment,
    title={Randaugment: Practical automated data augmentation with a reduced search space},
    author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
    pages={702--703},
    year={2020}
}

%%% Self-Distillation & Knowledge distillation
@inproceedings{toward_understanding,
    title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
    author={Zeyuan Allen-Zhu and Yuanzhi Li},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=Uuf2q9TfXGA}
}

%Self-Distillation
@inproceedings{born_again,
    title={Born again neural networks},
    author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
    booktitle={International Conference on Machine Learning},
    pages={1607--1616},
    year={2018},
    organization={PMLR}
}

@inproceedings{noisy_student,
    title={Self-training with noisy student improves imagenet classification},
    author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={10687--10698},
    year={2020}
}

@inproceedings{be_yourown_teacher,
    title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
    author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={3713--3722},
    year={2019}
}

@inproceedings{knowledge_distill,
    author = {Hinton, Geoffrey and Dean, Jeff and Vinyals, Oriol},
    year = {2014},
    month = {03},
    pages = {1-9},
    title = {Distilling the Knowledge in a Neural Network}
}

%Contrastive Knowledge distillation
@inproceedings{tian2020Contrastive,
    title={Contrastive Representation Distillation},
    author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SkgpBJrtvS}
}

%%% MULTIMODAL MODEL
@inproceedings{lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18123--18133},
  year={2022}
}

% CLIP model
@inproceedings{clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle={International conference on machine learning},
    pages={8748--8763},
    year={2021},
    organization={PMLR}
}

%BLIP
@inproceedings{blip-1,
    title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
    author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
    booktitle={International Conference on Machine Learning},
    pages={12888--12900},
    year={2022},
    organization={PMLR}
}

@article{blip-2,
    title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
    author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
    journal={arXiv preprint arXiv:2301.12597},
    year={2023}
}

%Align Model
@inproceedings{align,
    title={Scaling up visual and vision-language representation learning with noisy text supervision},
    author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
    booktitle={International Conference on Machine Learning},
    pages={4904--4916},
    year={2021},
    organization={PMLR}
}

%CoCa
@article{coca,
    title={CoCa: Contrastive Captioners are Image-Text Foundation Models},
    author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2022},
    url={https://openreview.net/forum?id=Ee277P3AYC},
    note={}
}

%BEIT
@inproceedings{beit-1,
    title={{BE}iT: {BERT} Pre-Training of Image Transformers},
    author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{beit-3,
  title={Image as a foreign language: Beit pretraining for vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19175--19186},
  year={2023}
}

%Flamingo
@article{flamingo,
    title={Flamingo: a visual language model for few-shot learning},
    author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={23716--23736},
    year={2022}
}

@article{albef,
    title={Align before fuse: Vision and language representation learning with momentum distillation},
    author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
    journal={Advances in neural information processing systems},
    volume={34},
    pages={9694--9705},
    year={2021}
}

@article{mplug,
    title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
    author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
    journal={arXiv preprint arXiv:2205.12005},
    year={2022}
}

@inproceedings{pali,
    title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
    author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=mWVoBz4W0u}
}

@article{vlmo,
    title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
    author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={32897--32912},
    year={2022}
}

@inproceedings{uniter,
title = {UNITER: UNiversal Image-TExt Representation Learning},
author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_7},
doi = {10.1007/978-3-030-58577-8_7},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {104–120},
numpages = {17},
location = {Glasgow, United Kingdom}
}


%%% VISION MODEL
% Transformer
@inproceedings{vit,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

%ResNet
@inproceedings{resnet,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

%Dino
@inproceedings{dino,
    title={Emerging properties in self-supervised vision transformers},
    author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    pages={9650--9660},
    year={2021}
}

@INPROCEEDINGS{wide_resnet,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

%%% Dataset
% ImageNet
@inproceedings{imagenet,
    title={Imagenet: A large-scale hierarchical image database},
    author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
    booktitle={2009 IEEE conference on computer vision and pattern recognition},
    pages={248--255},
    year={2009},
    organization={Ieee}
}

%Cifar
@article{cifar,
    title={Learning multiple layers of features from tiny images},
    author={Krizhevsky, Alex and Hinton, Geoffrey and others},
    year={2009},
    publisher={Toronto, ON, Canada}
}

%Foundation Model
@article{foundation_model,
    title={On the opportunities and risks of foundation models},
    author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
    journal={arXiv preprint arXiv:2108.07258},
    year={2021}
}

%s-clip
@article{s-clip,
    title={S-clip: Semi-supervised vision-language learning using few specialist captions},
    author={Mo, Sangwoo and Kim, Minkyu and Lee, Kyungmin and Shin, Jinwoo},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}

%multimodal summarization
@inproceedings{pseudo-summ,
    title = "Exploiting Pseudo Image Captions for Multimodal Summarization",
    author = "Jiang, Chaoya  and
        Xie, Rui  and
        Ye, Wei  and
        Sun, Jinan  and
        Zhang, Shikun",
    editor = "Rogers, Anna  and
        Boyd-Graber, Jordan  and
        Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.12",
    doi = "10.18653/v1/2023.findings-acl.12",
    pages = "161--175",
    abstract = "Multimodal summarization with multimodal output (MSMO) faces a challenging semantic gap between visual and textual modalities due to the lack of reference images for training. Our pilot investigation indicates that image captions, which naturally connect texts and images, can significantly benefit MSMO. However, exposure of image captions during training is inconsistent with MSMO{'}s task settings, where prior cross-modal alignment information is excluded to guarantee the generalization of cross-modal semantic modeling. To this end, we propose a novel coarse-to-fine image-text alignment mechanism to identify the most relevant sentence of each image in a document, resembling the role of image captions in capturing visual knowledge and bridging the cross-modal semantic gap. Equipped with this alignment mechanism, our method easily yet impressively sets up state-of-the-art performances on all intermodality and intramodality metrics (e.g., more than 10{\%} relative improvement on image recommendation precision). Further experiments reveal the correlation between image captions and text summaries, and prove that the pseudo image captions we generated are even better than the original ones in terms of promoting multimodal summarization.",
}

%learning by hallucination
@InProceedings{learn_hallucinate,
    author    = {Wang, Tzu-Jui Julius and Laaksonen, Jorma and Langer, Tomas and Arponen, Heikki and Bishop, Tom E.},
    title     = {Learning by Hallucinating: Vision-Language Pre-Training With Weak Supervision},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {1073-1083}
}

%zlap
@InProceedings{zlap,
    author    = {Stojni\'c, Vladan and Kalantidis, Yannis and Tolias, Giorgos},
    title     = {Label Propagation for Zero-shot Classification with Vision-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2024}
}

%cross-modal retrieval review paper
@article{cross_modal_review,
    title={A comprehensive survey on cross-modal retrieval},
    author={Wang, Kaiye and Yin, Qiyue and Wang, Wei and Wu, Shu and Wang, Liang},
    journal={arXiv preprint arXiv:1607.06215},
    year={2016}
}

%cross-modal retrieval review paper
@article{cross_modal_review_2,
    title = {Comparative analysis on cross-modal information retrieval: A review},
    journal = {Computer Science Review},
    volume = {39},
    pages = {100336},
    year = {2021},
    issn = {1574-0137},
    doi = {https://doi.org/10.1016/j.cosrev.2020.100336},
    url = {https://www.sciencedirect.com/science/article/pii/S1574013720304366},
    author = {Parminder Kaur and Husanbir Singh Pannu and Avleen Kaur Malhi},
    keywords = {Cross-modal, Multimedia, Information retrieval, Data fusion, Comparative analysis},
    abstract = {Human beings experience life through a spectrum of modes such as vision, taste, hearing, smell, and touch. These multiple modes are integrated for information processing in our brain using a complex network of neuron connections. Likewise for artificial intelligence to mimic the human way of learning and evolve into the next generation, it should elucidate multi-modal information fusion efficiently. Modality is a channel that conveys information about an object or an event such as image, text, video, and audio. A research problem is said to be multi-modal when it incorporates information from more than a single modality. Multi-modal systems involve one mode of data to be inquired for any (same or varying) modality outcome whereas cross-modal system strictly retrieves the information from a dissimilar modality. As the input–output queries belong to diverse modal families, their coherent comparison is still an open challenge with their primitive forms and subjective definition of content similarity. Numerous techniques have been proposed by researchers to handle this issue and to reduce the semantic gap of information retrieval among different modalities. This paper focuses on a comparative analysis of various research works in the field of cross-modal information retrieval. Comparative analysis of several cross-modal representations and the results of the state-of-the-art methods when applied on benchmark datasets have also been discussed. In the end, open issues are presented to enable the researchers to a better understanding of the present scenario and to identify future research directions.}
}

%medical and stellite image not good result
@misc{kim2024discovering,
    title={Discovering and Mitigating Visual Biases through Keyword Explanation}, 
    author={Younghyun Kim and Sangwoo Mo and Minkyu Kim and Kyungmin Lee and Jaeho Lee and Jinwoo Shin},
    year={2024},
    eprint={2301.11104},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%generate fashion caption
@inproceedings{generate_fashion_caption,
    Author = {Xuewen Yang and Heming Zhang and Di Jin and Yingru Liu and Chi-Hao Wu and Jianchao Tan and Dongliang Xie and Jue Wang and Xin Wang},
    Title = {Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards},
    booktitle = {ECCV},
    Year = {2020}
}

@article{mask_object,
  title={Data Efficient Masked Language Modeling for Vision and Language},
  author={Bitton, Yonatan and Stanovsky, Gabriel and Elhadad, Michael and Schwartz, Roy},
  journal={arXiv preprint arXiv:2109.02040},
  year={2021}
}

@article{selective_masking,
  title={Difference-masking: Choosing what to mask in continued pretraining},
  author={Wilf, Alex and Akter, Syeda Nahida and Mathur, Leena and Liang, Paul Pu and Mathew, Sheryl and Shou, Mengrou and Nyberg, Eric and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2305.14577},
  year={2023}
}