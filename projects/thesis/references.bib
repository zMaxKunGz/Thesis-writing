%%% Text Encoder model
%BERT
@article{bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

%%% SEMI SUPERVISED LEARNING
% Meta Pseudo Label
@inproceedings{meta_pseudo,
    title={Meta pseudo labels},
    author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={11557--11568},
    year={2021}
}

% VIT Semi Supervised
@inproceedings{semisup-vit,
    title={Semi-supervised Vision Transformers at Scale},
    author={Zhaowei Cai and Avinash Ravichandran and Paolo Favaro and Manchen Wang and Davide Modolo and Rahul Bhotika and Zhuowen Tu and Stefano Soatto},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=7a2IgJ7V4W}
}

@article{aug_consistency,
    title={Unsupervised data augmentation for consistency training},
    author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={6256--6268},
    year={2020}
}

%Temperal Ensembling
@article{laine2016temporal,
    title={Temporal ensembling for semi-supervised learning},
    author={Laine, Samuli and Aila, Timo},
    journal={arXiv preprint arXiv:1610.02242},
    year={2016}
}

%EMA Teacher
@article{mean_teacher,
    title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
    author={Tarvainen, Antti and Valpola, Harri},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

%EMAN Teacher
@inproceedings{eman,
    title={Exponential moving average normalization for self-supervised and semi-supervised learning},
    author={Cai, Zhaowei and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Tu, Zhuowen and Soatto, Stefano},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={194--203},
    year={2021}
}

%FixMatch
@article{fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

%Interpolation consistency training
@article{inter_consistency,
    title={Interpolation consistency training for semi-supervised learning},
    author={Verma, Vikas and Kawaguchi, Kenji and Lamb, Alex and Kannala, Juho and Solin, Arno and Bengio, Yoshua and Lopez-Paz, David},
    journal={Neural Networks},
    volume={145},
    pages={90--106},
    year={2022},
    publisher={Elsevier}
}

%MixUp
@article{mixup,
    title={mixup: Beyond empirical risk minimization},
    author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
    journal={arXiv preprint arXiv:1710.09412},
    year={2017}
}

%MixMatch
@article{mixmatch,
    title={Mixmatch: A holistic approach to semi-supervised learning},
    author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

%Unsupervised Data Augmentation
@inproceedings{cubuk2020randaugment,
    title={Randaugment: Practical automated data augmentation with a reduced search space},
    author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
    pages={702--703},
    year={2020}
}

%%% Self-Distillation & Knowledge distillation
@inproceedings{toward_understanding,
    title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
    author={Zeyuan Allen-Zhu and Yuanzhi Li},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=Uuf2q9TfXGA}
}

%Self-Distillation
@inproceedings{born_again,
    title={Born again neural networks},
    author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
    booktitle={International Conference on Machine Learning},
    pages={1607--1616},
    year={2018},
    organization={PMLR}
}

@inproceedings{noisy_student,
    title={Self-training with noisy student improves imagenet classification},
    author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={10687--10698},
    year={2020}
}

@inproceedings{be_yourown_teacher,
    title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
    author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={3713--3722},
    year={2019}
}

@inproceedings{knowledge_distill,
    author = {Hinton, Geoffrey and Dean, Jeff and Vinyals, Oriol},
    year = {2014},
    month = {03},
    pages = {1-9},
    title = {Distilling the Knowledge in a Neural Network}
}

%Contrastive Knowledge distillation
@inproceedings{tian2020Contrastive,
    title={Contrastive Representation Distillation},
    author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SkgpBJrtvS}
}

%%% MULTIMODAL MODEL
@inproceedings{lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18123--18133},
  year={2022}
}

% CLIP model
@inproceedings{clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle={International conference on machine learning},
    pages={8748--8763},
    year={2021},
    organization={PMLR}
}

%BLIP
@inproceedings{blip-1,
    title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
    author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
    booktitle={International Conference on Machine Learning},
    pages={12888--12900},
    year={2022},
    organization={PMLR}
}

@article{blip-2,
    title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
    author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
    journal={arXiv preprint arXiv:2301.12597},
    year={2023}
}

%Align Model
@inproceedings{align,
    title={Scaling up visual and vision-language representation learning with noisy text supervision},
    author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
    booktitle={International Conference on Machine Learning},
    pages={4904--4916},
    year={2021},
    organization={PMLR}
}

%CoCa
@article{coca,
    title={CoCa: Contrastive Captioners are Image-Text Foundation Models},
    author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2022},
    url={https://openreview.net/forum?id=Ee277P3AYC},
    note={}
}

%BEIT
@inproceedings{beit-1,
    title={{BE}iT: {BERT} Pre-Training of Image Transformers},
    author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{beit-3,
  title={Image as a foreign language: Beit pretraining for vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19175--19186},
  year={2023}
}

%Flamingo
@article{flamingo,
    title={Flamingo: a visual language model for few-shot learning},
    author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={23716--23736},
    year={2022}
}

@article{albef,
    title={Align before fuse: Vision and language representation learning with momentum distillation},
    author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
    journal={Advances in neural information processing systems},
    volume={34},
    pages={9694--9705},
    year={2021}
}

@article{mplug,
    title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
    author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
    journal={arXiv preprint arXiv:2205.12005},
    year={2022}
}

@inproceedings{pali,
    title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
    author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=mWVoBz4W0u}
}

@article{vlmo,
    title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
    author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={32897--32912},
    year={2022}
}

@inproceedings{uniter,
title = {UNITER: UNiversal Image-TExt Representation Learning},
author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_7},
doi = {10.1007/978-3-030-58577-8_7},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {104–120},
numpages = {17},
location = {Glasgow, United Kingdom}
}


%%% VISION MODEL
% Transformer
@inproceedings{vit,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

%ResNet
@inproceedings{resnet,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

%Dino
@inproceedings{dino,
    title={Emerging properties in self-supervised vision transformers},
    author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    pages={9650--9660},
    year={2021}
}

@INPROCEEDINGS{wide_resnet,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

%%% Dataset
% ImageNet
@inproceedings{imagenet,
    title={Imagenet: A large-scale hierarchical image database},
    author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
    booktitle={2009 IEEE conference on computer vision and pattern recognition},
    pages={248--255},
    year={2009},
    organization={Ieee}
}

%Cifar
@article{cifar,
    title={Learning multiple layers of features from tiny images},
    author={Krizhevsky, Alex and Hinton, Geoffrey and others},
    year={2009},
    publisher={Toronto, ON, Canada}
}

%Foundation Model
@article{foundation_model,
    title={On the opportunities and risks of foundation models},
    author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
    journal={arXiv preprint arXiv:2108.07258},
    year={2021}
}

%s-clip
@article{s-clip,
    title={S-clip: Semi-supervised vision-language learning using few specialist captions},
    author={Mo, Sangwoo and Kim, Minkyu and Lee, Kyungmin and Shin, Jinwoo},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}

%multimodal summarization
@inproceedings{pseudo-summ,
    title = "Exploiting Pseudo Image Captions for Multimodal Summarization",
    author = "Jiang, Chaoya  and
        Xie, Rui  and
        Ye, Wei  and
        Sun, Jinan  and
        Zhang, Shikun",
    editor = "Rogers, Anna  and
        Boyd-Graber, Jordan  and
        Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.12",
    doi = "10.18653/v1/2023.findings-acl.12",
    pages = "161--175",
    abstract = "Multimodal summarization with multimodal output (MSMO) faces a challenging semantic gap between visual and textual modalities due to the lack of reference images for training. Our pilot investigation indicates that image captions, which naturally connect texts and images, can significantly benefit MSMO. However, exposure of image captions during training is inconsistent with MSMO{'}s task settings, where prior cross-modal alignment information is excluded to guarantee the generalization of cross-modal semantic modeling. To this end, we propose a novel coarse-to-fine image-text alignment mechanism to identify the most relevant sentence of each image in a document, resembling the role of image captions in capturing visual knowledge and bridging the cross-modal semantic gap. Equipped with this alignment mechanism, our method easily yet impressively sets up state-of-the-art performances on all intermodality and intramodality metrics (e.g., more than 10{\%} relative improvement on image recommendation precision). Further experiments reveal the correlation between image captions and text summaries, and prove that the pseudo image captions we generated are even better than the original ones in terms of promoting multimodal summarization.",
}

%learning by hallucination
@InProceedings{learn_hallucinate,
    author    = {Wang, Tzu-Jui Julius and Laaksonen, Jorma and Langer, Tomas and Arponen, Heikki and Bishop, Tom E.},
    title     = {Learning by Hallucinating: Vision-Language Pre-Training With Weak Supervision},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {1073-1083}
}

%zlap
@InProceedings{zlap,
    author    = {Stojni\'c, Vladan and Kalantidis, Yannis and Tolias, Giorgos},
    title     = {Label Propagation for Zero-shot Classification with Vision-Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2024}
}

%cross-modal retrieval review paper
@article{cross_modal_review,
    title={A comprehensive survey on cross-modal retrieval},
    author={Wang, Kaiye and Yin, Qiyue and Wang, Wei and Wu, Shu and Wang, Liang},
    journal={arXiv preprint arXiv:1607.06215},
    year={2016}
}

%cross-modal retrieval review paper
@article{cross_modal_review_2,
    title = {Comparative analysis on cross-modal information retrieval: A review},
    journal = {Computer Science Review},
    volume = {39},
    pages = {100336},
    year = {2021},
    issn = {1574-0137},
    doi = {https://doi.org/10.1016/j.cosrev.2020.100336},
    url = {https://www.sciencedirect.com/science/article/pii/S1574013720304366},
    author = {Parminder Kaur and Husanbir Singh Pannu and Avleen Kaur Malhi},
    keywords = {Cross-modal, Multimedia, Information retrieval, Data fusion, Comparative analysis},
    abstract = {Human beings experience life through a spectrum of modes such as vision, taste, hearing, smell, and touch. These multiple modes are integrated for information processing in our brain using a complex network of neuron connections. Likewise for artificial intelligence to mimic the human way of learning and evolve into the next generation, it should elucidate multi-modal information fusion efficiently. Modality is a channel that conveys information about an object or an event such as image, text, video, and audio. A research problem is said to be multi-modal when it incorporates information from more than a single modality. Multi-modal systems involve one mode of data to be inquired for any (same or varying) modality outcome whereas cross-modal system strictly retrieves the information from a dissimilar modality. As the input–output queries belong to diverse modal families, their coherent comparison is still an open challenge with their primitive forms and subjective definition of content similarity. Numerous techniques have been proposed by researchers to handle this issue and to reduce the semantic gap of information retrieval among different modalities. This paper focuses on a comparative analysis of various research works in the field of cross-modal information retrieval. Comparative analysis of several cross-modal representations and the results of the state-of-the-art methods when applied on benchmark datasets have also been discussed. In the end, open issues are presented to enable the researchers to a better understanding of the present scenario and to identify future research directions.}
}

%medical and stellite image not good result
@misc{kim2024discovering,
    title={Discovering and Mitigating Visual Biases through Keyword Explanation}, 
    author={Younghyun Kim and Sangwoo Mo and Minkyu Kim and Kyungmin Lee and Jaeho Lee and Jinwoo Shin},
    year={2024},
    eprint={2301.11104},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%generate fashion caption
@inproceedings{generate_fashion_caption,
    Author = {Xuewen Yang and Heming Zhang and Di Jin and Yingru Liu and Chi-Hao Wu and Jianchao Tan and Dongliang Xie and Jue Wang and Xin Wang},
    Title = {Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards},
    booktitle = {ECCV},
    Year = {2020}
}

@article{mask_object,
    title={Data Efficient Masked Language Modeling for Vision and Language},
    author={Bitton, Yonatan and Stanovsky, Gabriel and Elhadad, Michael and Schwartz, Roy},
    journal={arXiv preprint arXiv:2109.02040},
    year={2021}
}

@article{selective_masking,
    title={Difference-masking: Choosing what to mask in continued pretraining},
    author={Wilf, Alex and Akter, Syeda Nahida and Mathur, Leena and Liang, Paul Pu and Mathew, Sheryl and Shou, Mengrou and Nyberg, Eric and Morency, Louis-Philippe},
    journal={arXiv preprint arXiv:2305.14577},
    year={2023}
}

@inproceedings{valse,
    title = "{VALSE}: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena",
    author = "Parcalabescu, Letitia  and
        Cafagna, Michele  and
        Muradjan, Lilitta  and
        Frank, Anette  and
        Calixto, Iacer  and
        Gatt, Albert",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.567",
    pages = "8253--8280",
    abstract = "We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V{\&}L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V{\&}L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V{\&}L models from a linguistic perspective, complementing the canonical task-centred V{\&}L evaluations.",
}

@inproceedings{mm-shap,
    title = "{MM}-{SHAP}: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models {\&} Tasks",
    author = "Parcalabescu, Letitia  and
        Frank, Anette",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.223",
    doi = "10.18653/v1/2023.acl-long.223",
    pages = "4032--4059",
    abstract = "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality.Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets.Experiments with six VL models {--} LXMERT, CLIP and four ALBEF variants {--} on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at https://github.com/Heidelberg-NLP/MM-SHAP.",
}

@inproceedings{lxmert,
    title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
    author={Tan, Hao and Bansal, Mohit},
    booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
    year={2019}
}

@inproceedings{attention-explanations,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}

@inproceedings{grad-cam,
    title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
    author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
    booktitle={Proceedings of the IEEE international conference on computer vision},
    pages={618--626},
    year={2017}
}

@article{amnesic-probing,
    author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
    title = "{Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {160-175},
    year = {2021},
    month = {03},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00359},
    url = {https://doi.org/10.1162/tacl\_a\_00359},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00359/1894330/tacl\_a\_00359.pdf},
}

@inproceedings{dime,
    author    = {Jeanneret, Guillaume and Simon, Lo\"ic and Fr\'ed\'eric Jurie},
    title     = {Diffusion Models for Counterfactual Explanations},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {December},
    year      = {2022}
}

@article{albert,
    title={Albert: A lite bert for self-supervised learning of language representations},
    author={Lan, Z},
    journal={arXiv preprint arXiv:1909.11942},
    year={2019}
}

@misc{dictbert,
      title={Dict-BERT: Enhancing Language Model Pre-training with Dictionary}, 
      author={Wenhao Yu and Chenguang Zhu and Yuwei Fang and Donghan Yu and Shuohang Wang and Yichong Xu and Michael Zeng and Meng Jiang},
      year={2022},
      eprint={2110.06490},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.06490}, 
}

@misc{opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@misc{realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}

@misc{posmaskinglearning,
      title={Learning Better Masking for Better Language Model Pre-training}, 
      author={Dongjie Yang and Zhuosheng Zhang and Hai Zhao},
      year={2023},
      eprint={2208.10806},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rf-curriculum-masking,
    title = "Curriculum Masking in Vision-Language Pretraining to Maximize Cross Modal Interaction",
    author = "Tou, Kraig  and
        Sun, Zijun",
    editor = "Duh, Kevin  and
        Gomez, Helena  and
        Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.203",
    doi = "10.18653/v1/2024.naacl-long.203",
    pages = "3672--3688",
    abstract = "Many leading methods in Vision and language (V+L) pretraining utilize masked language modeling (MLM) as a standard pretraining component, with the expectation that reconstruction of masked text tokens would necessitate reference to corresponding image context via cross/self attention and thus promote representation fusion. However, we observe that the minimization of MLM loss in earlier training stages can depend disproportionately on local text signals, leading to poor training efficiency and inconsistency with the goal of representation fusion. The extent of this lack of cross modal interaction depends strongly which token(s) are masked. To address this issue, we propose a curriculum masking scheme as a replacement for random masking. Tokens are selected to be masked at a frequency proportional to the expected level of cross modal interaction necessary to reconstruct them. This is achieved using a parallel mask selection agent that measures the cross modal flow of information and treats it as a reward to be maximized. By additionally masking contiguous spans that include key objects and their relations, we also achieve better relational understanding, which has been shown to be lacking in many SOTA models. Our experiments on a wide range of V+L tasks show that we trail closely behind state-of-the-art methods despite pretraining on 300x to 1000x less data and we also achieve either top or runner-up performance on tasks from the ARO benchmark which tests compositional relationships. Finally, we demonstrate the potential of our method to scale to larger pretraining data.",
}

@inproceedings{foil-dataset,
    title={"FOIL it! Find One mismatch between Image and Language caption"},
    author={Shekhar, Ravi and Pezzelle, Sandro and Klimovich, Yauhen and Herbelot, Aurelie and Nabi, Moin and Sangineto, Enver and Bernardi, Raffaella},
    booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 1: Long Papers)},
    pages     = {255--265},
    year={2017}
}

@ARTICLE{vl-review,
    author={Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
    title={Vision-Language Models for Vision Tasks: A Survey}, 
    year={2024},
    volume={46},
    number={8},
    pages={5625-5644},
    keywords={Task analysis;Visualization;Training;Deep learning;Surveys;Data models;Predictive models;Big Data;big model;deep learning;deep neural network;knowledge distillation;object detection;pre-training;semantic segmentation;transfer learning;vision-language model;visual recognition;image classification},
    doi={10.1109/TPAMI.2024.3369699}
}

@misc{medclip,
    title={MedCLIP: Contrastive Learning from Unpaired Medical Images and Text}, 
    author={Zifeng Wang and Zhenbang Wu and Dinesh Agarwal and Jimeng Sun},
    year={2022},
    eprint={2210.10163},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2210.10163}, 
}

@article{spanBERT,
    title = "{S}pan{BERT}: Improving Pre-training by Representing and Predicting Spans",
    author = "Joshi, Mandar  and
        Chen, Danqi  and
        Liu, Yinhan  and
        Weld, Daniel S.  and
        Zettlemoyer, Luke  and
        Levy, Omer",
    editor = "Johnson, Mark  and
        Roark, Brian  and
        Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.5",
    doi = "10.1162/tacl_a_00300",
    pages = "64--77",
    abstract = "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\%} F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1",
}

@inproceedings{n-gram-masking,
    title={{\{}PMI{\}}-Masking: Principled masking of correlated spans},
    author={Yoav Levine and Barak Lenz and Opher Lieber and Omri Abend and Kevin Leyton-Brown and Moshe Tennenholtz and Yoav Shoham},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=3Aoft6NWFej}
}

@article{ERNIE,
    author       = {Yu Sun and
                    Shuohuan Wang and
                    Yu{-}Kun Li and
                    Shikun Feng and
                    Xuyi Chen and
                    Han Zhang and
                    Xin Tian and
                    Danxiang Zhu and
                    Hao Tian and
                    Hua Wu},
    title        = {{ERNIE:} Enhanced Representation through Knowledge Integration},
    journal      = {CoRR},
    volume       = {abs/1904.09223},
    year         = {2019}
}

@article{resnext,
    title={Aggregated Residual Transformations for Deep Neural Networks},
    author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
    journal={arXiv preprint arXiv:1611.05431},
    year={2016}
}