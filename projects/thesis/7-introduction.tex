% set 0.5 inch indentation
\setlength{\parindent}{0in}
% set paragraph space = 0 space
\setlength{\parskip}{1.5mm}
% set line space 1.5
\setlength{\baselineskip}{1.6em}

\chapter{INTRODUCTION}
\section{Background}
\acrfull{vl} models have gained significant attention due to their ability to perform both zero-shot and transfer learning, achieving high performance across numerous downstream tasks through pre-training with web-scale image-text pairs \cite{s-clip, medclip, vl-review}.  
Many \acrshort{vl} models incorporated \acrfull{mlm} as a pre-training task, making it an important method to train \acrshort{vl} models \cite{albef, mplug, uniter, beit-3, lxmert}.  
Typically, a subset of word tokens is randomly masked at a percentage during training, and the model is tasked with predicting these masked tokens using information from both visual and language modalities.  
This masking approach has proven to enhance the alignment between visual and linguistic representations, boosting performance in \acrshort{vl} tasks \cite{lxmert}. 

Despite the widespread adoption of \acrshort{mlm} in \acrshort{vl} training, the effects of masking tokens based on sentence structure remain underexplored.
Prior work has shown that effectiveness increases when the masked tokens are chosen to be semantically informative. 
For example, masking object words yields clear gains over random masking \cite{mask_object}; selectively masking infrequent words improves out-of-domain generalization during continued pre-training \cite{selective_masking}; and curriculum-based masking reduces shallow reliance on local cues and promotes more consistent cross-modal interactions \cite{rf-curriculum-masking}. 
These findings emphasize the importance of strategic token selection in \acrshort{mlm} to enhance \acrshort{vl} model performance and efficiency.

In this work, we aim to address the gap in understanding how masking each \acrfull{pos} impacts \acrshort{vl} models inspired by how human interpret the world through language, where each part of speech serves a distinct purpose.
By selectively masking different parts of speech, we can better understand how each \acrshort{pos} category affects the alignment between visual and linguistic information.
This also allows us to probe what information the model can infer beyond the masked word itself.
To further explore the effect of each \acrshort{pos}, training without the \acrshort{mlm} task and with different \acrshort{pos} masking probabilities are compared.

The experiment is designed to answer the following questions:
\begin{enumerate}  
    \item How does masking each \acrshort{pos} affect the performance and training loss of \acrshort{vl} models during pre-training, and how does it influence downstream performance on \acrfull{vqa}?
    \item What underlying representations do \acrshort{vl} models acquire through \acrshort{mlm} training, and does this process enable them to learn more than the masked word itself?
    \item What is the difference between training without the \acrshort{mlm} task compared to training with it, and when masking each \acrshort{pos} with a 100 percent masking ratio?
\end{enumerate}

The main contributions of this thesis are fourfold.
First, we present a systematic study of \acrshort{pos} masking strategies in \acrshort{vl} pre-training, offering new insights into how each \acrshort{pos} category contributes to cross-modal alignment.  
Second, we benchmark the effects of different \acrshort{pos} masking strategies across retrieval and \acrshort{vqa} tasks, identifying when and where specific linguistic categories are most influential.  
Third, we compare models trained with no \acrshort{mlm}, standard random masking, and 100\% \acrshort{pos}-specific masking, providing a deeper understanding of the role of \acrshort{mlm} in both retrieval and \acrshort{vqa}.  
Finally, our findings offer insights for developing more efficient and linguistically informed masking strategies in future \acrshort{vl} systems.  



\section{Scope}  
The scope of this thesis is defined as follows:  
\begin{enumerate}  
    \item The training and testing datasets are web-scale image–text pairs.  
    \item The scope is limited to cross-attention vision–language models trained with \acrshort{mlm}, \acrshort{itm}, and \acrshort{itc} tasks, which are widely adopted objectives in modern \acrshort{vl} pre-training.
    \item This study concentrates on structured masking guided by parts of speech, specifically focusing on nouns, verbs, adjectives, adverbs, proper nouns, determiners, auxiliaries, pronouns, and adpositions.
\end{enumerate}  