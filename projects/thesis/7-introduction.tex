% set 0.5 inch indentation
\setlength{\parindent}{0in}
% set paragraph space = 0 space
\setlength{\parskip}{1.5mm}
% set line space 1.5
\setlength{\baselineskip}{1.6em}

\chapter{INTRODUCTION}
\section{Background}
Human beings describe and interpret the world through language, where each part of speech serves a distinct purpose: nouns ground entities, verbs express actions, and adjectives refine attributes. 
For example, we can easily imagine the scene described by the sentence “a dog chases a ball” because its words are organized into a structured form that maps directly onto real-world concepts. 
Motivated by this ability, researchers have developed \acrfull{vl} models that aim to capture similar grounding by learning from large collections of paired images and text. 
These models attempt to bridge words and vision in the way humans naturally do.

One important training method in many \acrshort{vl} models is \acrfull{mlm}, in which a random set of tokens is masked and the model must predict the masked tokens using both visual and textual information. 
Prior work has shown that effectiveness increases when the masked tokens are chosen to be semantically informative. 
For example, masking object words yields clear gains over random masking \cite{mask_object}; selectively masking infrequent words improves out-of-domain generalization during continued pre-training \cite{selective_masking}; and curriculum-based masking reduces shallow reliance on local cues and promotes more consistent cross-modal interactions \cite{rf-curriculum-masking}. 
These findings emphasize the importance of strategic token selection in \acrshort{mlm} to enhance \acrshort{vl} model performance and efficiency.

Despite the widespread adoption of \acrshort{mlm} in \acrshort{vl} training, the effects of masking tokens based on sentence structure remain underexplored.
In this work, we aim to address the gap in understanding how masking each \acrfull{pos} impacts \acrshort{vl} models.
By selectively masking different parts of speech, we can better understand how each \acrshort{pos} category affects the alignment between visual and linguistic information.
This also allows us to probe what information the model can infer beyond the masked word itself, similar to how humans infer a missing word from surrounding context while simultaneously forcing human to understand the surrounding context.
To further explore the effect of each \acrshort{pos}, training without the \acrshort{mlm} task and with different \acrshort{pos} masking probabilities are compared.

The experiment is designed to answer the following questions:  
\begin{enumerate}  
    \item How does masking each \acrshort{pos} affect the performance and training loss of \acrshort{vl} models during pre-training, and how does it influence downstream performance on \acrfull{vqa}?
    \item What underlying representations do \acrshort{vl} models acquire through \acrshort{mlm} training, and does this process enable them to learn more than the masked word itself?
    \item What is the difference between training without the \acrshort{mlm} task compared to training with it, and when masking each \acrshort{pos} with a 100 percent masking ratio?
\end{enumerate}

\section{Contributions}  
The main contributions of this thesis are summarized as follows:  
\begin{enumerate}  
    \item We present a systematic study of \acrshort{pos} masking strategies in \acrshort{vl} pre-training, providing new insights into how each \acrshort{pos} contribute to cross-modal alignment. 
    \item We benchmark the effects of different \acrshort{pos} masking strategies across retrieval and visual question answering tasks, highlighting when and where specific linguistic categories are most influential. 
    \item We compare models trained with no \acrshort{mlm}, standard random masking, and 100\% \acrshort{pos}-specific masking, offering a deeper understanding of the effect of \Acrshort{mlm} in both retrieval task and \acrshort{vqa} task. 
    \item The results provide valuable insights for designing more efficient and linguistically informed masking strategies in future \acrshort{vl} systems.
\end{enumerate}  

\section{Scope}  
The scope of this thesis is defined as follows:  
\begin{enumerate}  
    \item The training and testing datasets are web-scale image–text pairs.  
    \item The scope is limited to cross-attention vision–language models trained with \acrshort{mlm}, \acrshort{itm}, and \acrshort{itc} tasks, which are widely adopted objectives in modern \acrshort{vl} pre-training.
    \item This study concentrates on structured masking guided by parts of speech, specifically focusing on nouns, verbs, adjectives, adverbs, proper nouns, determiners, auxiliaries, pronouns, and adpositions.
\end{enumerate}  