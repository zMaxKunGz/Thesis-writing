% set 0.5 inch indentation
\setlength{\parindent}{0in}
% set paragraph space = 0 space
\setlength{\parskip}{1.5mm}
% set line space 1.5
\setlength{\baselineskip}{1.6em}

\chapter{INTRODUCTION}
\section{Background}
Human beings describe and interpret the world through language, where each part of speech serves a distinct purpose: nouns ground entities, verbs express actions, and adjectives refine attributes. 
For example, we can easily imagine the scene described by the sentence “a dog chases a ball” because its words are organized into a structured form that maps directly onto real-world concepts. 
Motivated by this ability, researchers have developed \acrfull{vl} models that aim to capture similar grounding by learning from large collections of paired images and text \cite{clip,uniter}.
These models attempt to bridge words and vision in the way humans naturally do.

One important training method in many \acrshort{vl} models is \acrfull{mlm}, in which a random set of tokens is masked and the model must predict the masked tokens using both visual and textual information \cite{albef, mplug, uniter, beit-3}. 
Prior work has shown that effectiveness increases when the masked tokens are chosen to be semantically informative. 
For example, masking object words yields clear gains over random masking \cite{mask_object}; selectively masking infrequent words improves out-of-domain generalization during continued pre-training \cite{selective_masking}; and curriculum-based masking reduces shallow reliance on local cues and promotes more consistent cross-modal interactions \cite{rf-curriculum-masking}. 
These findings emphasize the importance of strategic token selection in \acrshort{mlm} to enhance \acrshort{vl} model performance and efficiency.

In this work, we aim to address the gap in understanding how masking each \acrfull{pos} impacts \acrshort{vl} models inspired by how human interpret the world through language, where each part of speech serves a distinct purpose.
By selectively masking different parts of speech, we can better understand how each \acrshort{pos} category affects the alignment between visual and linguistic information.
This also allows us to probe what information the model can infer beyond the masked word itself.
To further explore the effect of each \acrshort{pos}, training without the \acrshort{mlm} task and with different \acrshort{pos} masking probabilities are compared.

The experiment is designed to answer the following questions:
\begin{enumerate}  
    \item How does masking each \acrshort{pos} affect the performance and training loss of \acrshort{vl} models during pre-training, and how does it influence downstream performance on \acrfull{vqa}?
    \item What underlying representations do \acrshort{vl} models acquire through \acrshort{POS} masking training, and does this process enable them to learn more than the masked word itself?
    \item What is the difference between training without the \acrshort{mlm} task compared to training with it, and when masking each \acrshort{pos} with a 100 percent masking ratio?
\end{enumerate}

The main contributions of this thesis are fourfold.
First, we present a systematic study of \acrshort{pos} masking strategies in \acrshort{vl} pre-training, offering new insights into how each \acrshort{pos} category contributes to cross-modal alignment.  
Second, we benchmark the effects of different \acrshort{pos} masking strategies across retrieval and \acrshort{vqa} tasks, identifying when and where specific linguistic categories are most influential.  
Third, we compare models trained with no \acrshort{mlm}, standard random masking, and 100\% \acrshort{pos}-specific masking, providing a deeper understanding of the role of \acrshort{mlm} in both retrieval and \acrshort{vqa}.  
Finally, our findings offer insights for developing more efficient and linguistically informed masking strategies in future \acrshort{vl} systems.  



\section{Scope}  
The scope of this thesis is defined as follows:  
\begin{enumerate}  
    \item The training and testing datasets are web-scale image–text pairs.  
    \item The scope is limited to cross-attention \acrshort{vl} models trained with \acrshort{mlm}, \acrshort{itm}, and \acrshort{itc} tasks, which are widely adopted objectives in modern \acrshort{vl} pre-training.
    \item This study concentrates on structured masking guided by \Acrshort{pos}, specifically focusing on nouns, verbs, adjectives, adverbs, proper nouns, determiners, auxiliaries, pronouns, and adpositions.
\end{enumerate}  