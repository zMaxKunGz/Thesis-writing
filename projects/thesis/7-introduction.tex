% set 0.5 inch indentation
\setlength{\parindent}{0in}
% set paragraph space = 0 space
\setlength{\parskip}{1.5mm}
% set line space 1.5
\setlength{\baselineskip}{1.6em}

\chapter{INTRODUCTION}
\section{Background}
\acrfull{vl} models have gained significant attention due to their ability to perform both zero-shot and transfer learning, achieving high performance across numerous downstream tasks through pre-training with web-scale image-text pairs \cite{s-clip, medclip, vl-review}.  
Many \acrshort{vl} models incorporated \acrfull{mlm} as a pre-training task, making it an important method to train \acrshort{vl} models \cite{albef, mplug, uniter, beit-3, lxmert}.  
Typically, a subset of word tokens is randomly masked at a percentage during training, and the model is tasked with predicting these masked tokens using information from both visual and language modalities.  
This masking approach has proven to enhance the alignment between visual and linguistic representations, boosting performance in \acrshort{vl} tasks \cite{lxmert}.  

Despite the widespread adoption of \acrshort{mlm} in \acrshort{vl} training, its effects on model performance, efficiency, and training loss remain underexplored.  
\citeA{mask_object} demonstrated that many of the randomly masked tokens are often stop-words or punctuation, which the model can easily learn without any need for masking.  
Another study by \citeA{selective_masking} demonstrated that selectively masking infrequent words from the pre-training dataset can boost model performance on out-of-domain datasets during continued training.  
Additionally, \citeA{rf-curriculum-masking} suggested that random masking causes the model to rely heavily on local text signals, and it results in inefficient and inconsistent interactions between modalities, leading to suboptimal performance.  
These findings emphasize the importance of strategic token selection in \acrshort{mlm} to enhance \acrshort{vl} model performance and efficiency.  

In this work, we aimed to address the gap in understanding how masking each \acrfull{pos} impacts \acrshort{vl} models.  
Each \acrshort{pos} contributes distinctively to sentence meaning: nouns typically denote objects, while verbs describe actions and often demand contextual comprehension.  
By selectively masking different parts of speech, we could better understand how each \acrshort{pos} category affects the alignment between visual and linguistic information.  
To further explore the effect of each \acrshort{pos}, we experimented with fine-tuning the pre-trained model by masking each \acrshort{pos} category to assess the effect in the continued training situation.  
The experiment is designed to answer the following questions:  
\begin{enumerate}  
    \item How does masking each \acrshort{pos} impact the performance, efficiency, and training loss of \acrshort{vl} pre-training models?  
    \item How does each \acrshort{pos} masking strategy affect \acrfull{vqa} performance when analysed based on different question types?  
    \item What are the effects of part-of-speech masking during the fine-tuning phase of pre-trained \acrshort{vl} models?  
\end{enumerate}  


\section{Objective}  
The objectives for our experiment are as listed.  
\begin{enumerate}  
    \item Develop a pre-trained \acrshort{vl} model to evaluate the impact of masking each \acrshort{pos} on performance and training efficiency.  
    \item Benchmark the performance of each \acrshort{pos} masking approach using specialized datasets to gain a deeper understanding of masking effects with retrieval and question answering tasks.  
    \item Fine-tune the pre-trained \acrshort{vl} model to assess the performance and efficiency in the continued training situation.  
\end{enumerate}  

\section{Scope}
\begin{enumerate}  
    \item The training and testing datasets are web-scale image-text pairs.  
    \item The model architecture is a cross-attention model, chosen for its ability to jointly predict answers based on information from multiple modalities.  
\end{enumerate}  
