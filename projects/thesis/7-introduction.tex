% set 0.5 inch indentation
\setlength{\parindent}{0in}
% set paragraph space = 0 space
\setlength{\parskip}{1.5mm}
% set line space 1.5
\setlength{\baselineskip}{1.6em}

\chapter{INTRODUCTION}
\section{Background}
\acrfull{vl} models have gained significant attention due to their ability to perform both zero-shot and transfer learning, achieving high performance across numerous downstream tasks through pre-training with web-scale image-text pairs.
Many \acrshort{vl} models incorporate \acrfull{mlm} as a pre-training task, making it a fundamental approach for training \acrshort{vl} models \cite{albef, mplug, uniter, beit-3}.
Typically, a subset of word tokens is randomly masked at a fixed percentage during training, and the model is tasked with predicting these masked tokens using information from the visual modality.
This masking approach has proven to enhance the alignment between visual and linguistic representations, significantly boosting performance in vision-language tasks.

Despite the widespread adoption of \acrshort{mlm} in \acrshort{vl} training, its effects on model performance and learning dynamics remain underexplored.
\citeA{mask_object} demonstrated that many of the randomly masked tokens are often stop-words or punctuation, leading the model to rely on linguistic patterns that do not require visual understanding.
To address this, they proposed masking object-related tokens, which led to notable improvements in model performance and training efficiency compared to random masking \cite{mask_object}.
Another stduy by \citeA{selective_masking} demonstrated that selectively masking infrequent words from the pre-training dataset can boost model performance on out-of-domain datasets during continued training.
Additionally, \citeA{rf-curriculum-masking} introduced a curriculum-based masking strategy, where a reinforcement learning agent dynamically selects masking spans based on cross-model interactions. This approach improved the model's relational understanding while reducing the dataset size required for effective training.
These findings emphasize the importance of strategic token selection in \acrshort{mlm} to enhance \acrshort{vl} model learning and efficiency.

In order to explore the effect of \acrshort{mlm}, we experiment with masking based on part of speech as shown in Figure~\ref{fig:overview}.

As each part-of-speech contributes uniquely to the meaning of a sentence, for instance, nouns typically represent objects, while verbs describe actions, which often require contextual understanding, 
We hypothesize that masking verbs is the best way to increase the VL model understanding, as verbs in a sentence represent interactions between objects and require the model to rely more on visual information.
By selectively masking different part-of-speech, we can gain insight about how each part-of-speech category affects the alignment between vision and language modalities. 
The experiment is designed to answer the following questions:
\begin{enumerate}
    \item How does selective part-of-speech masking affect the alignment of vision and language modalities?
    \item How does part-of-speech masking change the contribution of the vision and language modalities to the model's output?
    \item How does specific masking impact the performance of vision question answering tasks, particularly in terms of improvements based on the type of question?
\end{enumerate}

\begin{figure}[h]
    \caption{Overall methodology}
    \label{fig:overview}
    Pre-training model with MLM task by masking token based on part-of-speech of the image captions.
    \begin{center}
        \includegraphics[width=0.6\textwidth]{Images/overview.png}
    \end{center}
    \small
\end{figure}

\section{Objective}
The objectives for our experiment are as listed.
\begin{enumerate}
    \item Pre-trained VL model for the experiment to identify the performance of masking in each part-of-speech.
    \item Benchmarking our method against specialize dataset based on linguistic feature \cite{valse} for better understanding of the masking effect.
    \item Analyze contribution from each modality of vision and language to the prediction output based on MM-Shap \cite{mm-shap}.
\end{enumerate}

\section{Scope}
\begin{enumerate}
    \item The training and testing dataset are both natural image.
    \item The model architecture is cross-attention model due to the ability of cross attention to jointly predicted answer based on another modality.
\end{enumerate}


