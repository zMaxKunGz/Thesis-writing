\chapter{DISCUSSION}
This chapter presents a discussion of our experimental results and reflects on their implications in relation to the main research questions: How does masking each \acrshort{pos} affect the performance and training loss of \acrshort{vl} models during pre-training, and how does it influence downstream performance on \acrshort{vqa}?
We begin by examining how masking different \acrshort{pos} categories impacts model performance during both pre-training and downstream evaluation.
We then discuss the results on \acrshort{vqa}, followed by an analysis of the no-masking baseline.
Finally, we outline the limitations of our approach.

\section{The Effect of Each \acrshort{pos} in Visual Pre-training}
Our experimental findings highlight that masking different \acrshort{pos} during pre-training leads to distinct outcomes.
On the Flickr30K benchmark, DET masking achieved the highest retrieval performance across both \acrshort{tr} and \acrshort{ir} tasks, followed by NOUN masking among the non-functional POS group.
Intuitively, one might expect nouns typically carrying more semantic should outperform determiners.
However, the results reveal the opposite, suggesting that determiners are easier for the model to learn.
When combined with the loss curves, we observe that retrieval performance correlates more closely with the \acrshort{itm} and \acrshort{itc} objectives than with the \acrshort{mlm} task.
The ranking of each \acrshort{pos} masking method aligns with the \acrshort{itm} and \acrshort{itc} loss curves, where lower loss corresponds to higher retrieval accuracy.
This suggests that for retrieval tasks, masking simpler tokens such as determiners may reduce the burden on the \acrshort{mlm} task and lead to better performance.

\section{The Effect of Each \acrshort{pos} in Based on Linguistic Phenoma VALSE Benchmark}
The VALSE benchmark evaluates a model’s fine-grained linguistic understanding.
We find that selectively masking specific \acrshort{pos} categories consistently outperforms random masking, and that each \acrshort{pos}-based masking strategy also performs well on tasks related to its corresponding linguistic functional.
This suggests that the \acrshort{mlm} objective plays a particularly important role in tasks requiring fine-grained understanding, and that performance is highly sensitive to which tokens are masked, highlighting the importance of strategic token selection.

Furthermore, We also observe that \acrshort{pos} masking captures more than just the task directly tied to its linguistic function.
For example, training the model with VERB and ADJ masks also yields strong performance on the counting task.
From the results, there remains a clear opportunity to improve performance by leveraging the strengths of each \acrshort{pos} category into a single model.

\section{Visual Question Answering}
In the VQA task, we observe that even some \acrshort{pos} categories that performed poorly in retrieval still enable the model to retain fine-grained image understanding.
Specifically, most non-functional \acrshort{pos} masking methods lead to better performance compared to functional \acrshort{pos} masking.
This result emphasizes that masking more content words leads to better fine-grained alignment.
If we combine the results of VALSE with \acrshort{vqa}, we can see that performance on the VALSE dataset is directly related to performance on the \acrshort{vqa} dataset, as both tasks require fine-grained understanding.

\section{Masking Probability}
When compared to training without masking, models trained without \acrshort{mlm} may perform reasonably well in a zero-shot setting.
However, models trained with the \acrshort{mlm} objective show clear improvements when fine-tuned on specific tasks.
This highlights the importance of \acrshort{mlm} for \acrshort{vl} model improvement.

\section{Limitations}
The limitation in term of dataset this study is the imbalance in \acrshort{pos} distribution, as observed in the \acrshort{pos} histogram.
Certain \acrshort{pos} categories, such as nouns, appear far more frequently than others, which may introduce bias in the model’s learning process and affect the generalizability of the results.
While this imbalance reflects the natural distribution of language in real-world datasets, it may confound our interpretation of how each \acrshort{pos} contributes to \Acrshort{vl} learning.
Another factor is that the performance of our work may not be optimal, since we deliberately adopted standard methods to ensure fair comparison.
Additionally, our experiments are limited to a specific set of benchmarks. 
Evaluation the effects of \acrshort{pos} masking across a broader range of datasets would be necessary to confirm the consistency and robustness of the observed phenomena.

\chapter{CONCLUSION}
\section{Conclusion}
In this work, we systematically investigate \acrshort{pos} masking strategies in \Acrshort{vl} pre-training and their effects on cross-modal alignment, retrieval, and VQA tasks.
Our findings show that different \acrshort{pos} categories yield distinct outcomes: for retrieval, simpler tokens such as determiners reduce the burden on the MLM objective and lead to higher accuracy, while for fine-grained benchmarks like VALSE and VQA, masking strategies aligned with linguistic functionals consistently outperform random masking and reveal strong sensitivity to token selection.
Notably, even \acrshort{pos} categories that perform poorly in retrieval still enhance fine-grained understanding, with content-word masking contributing most to alignment.
Moreover, models trained with MLM consistently surpass those without it when fine-tuned, underscoring the crucial role of \acrshort{mlm} in VL model improvement.
Together, these findings not only deepen our understanding of linguistic contributions in \acrshort{vl} pre-training but also show opportunities for designing more effective and linguistically grounded models.

\section{Future work}
Building on our findings, we identify a clear gap in fully leveraging \acrshort{pos} masking strategies in combination to enhance model performance.
Future work could explore adaptive masking approaches that integrate multiple \acrshort{pos} categories, selecting them dynamically to suit different tasks.
In addition, adjusting loss scheduling presents another promising direction, as our results suggest that certain tasks depend more heavily on specific objectives.

Due to our scope, we focus only on a single POS to ensure fairness and comparability.
However, further research following a curriculum-based approach across multiple POS categories could be beneficial.
By progressively combining words from different POS categories to build more simpler sentence or a increase complexity of the sentences.
