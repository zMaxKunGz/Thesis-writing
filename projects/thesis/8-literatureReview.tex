\chapter{LITERATURE REVIEW}
This section of the literature review is organized around two key topics relevant to our study.
The first topic addresses \acrshort{vl} models, providing an overview of the model architectures recently used in \acrshort{vl} models and discussing the choice of the base architecture for the \acrshort{vl} model used in this research.
The second topic \acrshort{mlm}, an important pre-training approach that has improved \acrshort{vl} model performance.
Together, these sections provide a comprehensive overview of the methodological foundations of this study.

\section{Vision-Language model}
In the early stage of \acrshort{vl} learning, the goal of training is to align fine-grained features of the image with text. 
Many works have adopted object detection to create fine-grained labels for the training images \cite{uniter, vlmo}. 
However, the focus of \acrshort{vl} training has shifted to using web-scale image-text pairs as a training set, demonstrating competitive performance, as shown by CLIP \cite{clip}. 
\citeA{clip} proposed contrastive training for \acrshort{vl} with a large-scale image-text pairs dataset by optimizing the alignment of image and text encodings from the same pair, which was proven to be scalable by \citeA{align}. 
This approach has become a foundational model for \acrshort{vl} tasks \cite{foundation_model}.

Recent advancements in \acrshort{vl} model training can be roughly categorized into three main methods.  
The first approach is a separate unimodal encoder for each modality, as seen in models like CLIP \cite{clip} and ALIGN \cite{align}. 
This method is trained with the objective of aligning the intermediate outputs of each modality's encoding.  
The second method uses a cross-attention layer to fuse multimodal inputs, e.g., Flamingo \cite{flamingo}, mPLUG \cite{mplug}, LXMERT \cite{lxmert}, and ALBEF \cite{albef}. 
The cross-attention layer enables the model to fuse each modality more deeply.  
Finally, the third approach uses a single large attention model with concatenated image and text tokens as input, as in BEIT-3 \cite{beit-3}, OSCAR \cite{oscar}. 
This approach allows for early-stage fusion of each modality, though it requires the highest amount of computational resources.  
In this work, we adopt the cross-attention method as the base model due to its effectiveness in fusing multimodal inputs. 
Additionally, this approach allows the model to be trained using the \acrshort{mlm} task.

\section{Masked Language Modelling}
\acrshort{mlm} is a widely used pre-training method in language model (LM) training \cite{bert, albert, dictbert, opt, realm} as a self-supervised task. 
BERT \cite{bert} proposed \acrshort{mlm} as a pre-training task, which has been proven effective for pre-training language models. 
The \acrshort{mlm} task involves replacing some input tokens with a special [MASK] token, and the model must predict the masked tokens based on the given unmasked tokens. 
In the field of \acrshort{vl} models, many \acrshort{vl} models have also adopted \acrshort{mlm} as a training task to train the model to predict masked text based on visual information \cite{albef, mplug, uniter, beit-3}.

In the field of selective masking strategies in natural language processing, several works have further refined \acrshort{mlm} to enhance training efficiency. 
ERNIE \cite{ERNIE}, SpanBERT \cite{spanBERT}, and \(n\)-gram Masking \cite{n-gram-masking} propose span masking instead of single-token masking, which forces the model to rely more on long-range dependencies rather than adjacent tokens, resulting in better performance compared to BERT \cite{bert}. 
Considering linguistic features, \citeA{posmaskinglearning} conducted a training analysis based on POS masking focused on LM training. 
The results showed that focusing the masking of non-function words (ADJ, ADV, NOUN, PROPN, and VERB) in the later stages of training can encourage the LM model to develop a better contextual understanding.

For selective masking in \acrshort{vl} training, \citeA{mask_object} introduced an object token masking strategy, selectively masking object tokens in image captions and pre-training the model. 
This approach achieved superior performance compared to random masking. 
Another study by \citeA{selective_masking} showed that selectively masking infrequent words from the pre-training dataset during continued training enhances model performance on out-of-domain datasets. 
Additionally, \cite{rf-curriculum-masking} proposed a curriculum-based masking strategy in which a reinforcement learning agent dynamically selects masking spans based on cross-modal interactions. 
This method improved the model’s mulitmodalities understanding while reducing the dataset size needed for effective training. 
In this work, we conduct experiments to analyze the impact of each \acrshort{pos} on results within a \acrshort{vl} setting.

% \section{Part of speech tagging}

% \section{Probing and interpretability}
% In order to analyze deep learning models, it is important to utilize both probing and interpretation methods. 
% We can divide model-agnostic methods into two categories: white-box methods, which leverage the internal structure and parameters of deep learning models to interpret the reasoning behind their outputs, and black-box methods, where we modify the input and observe the resulting changes in the model's output.

% For white-box methods, Grad-CAM \citeA{grad-cam} is a technique used with CNNs and Vision Transformers. 
% This method considers the gradient of the input to interpret the reasoning behind the model’s prediction. 
% \citeA{attention-explanations} proposed a method for interpreting attention scores. 
% However, these white-box methods are not well-suited for analyzing multimodal deep learning models to understand interactions between different modalities.

% For black-box methods, DIME \cite{dime} proposed an interpretation technique that probes a deep learning model using an interpretable linear function over the model’s output. 
% Another method to consider is Amnesic Probing \cite{amnesic-probing}, which uses causal intervention by removing parts of the input. 
% This allows us to measure the contribution of each part of the input to the model's output. 
% In the context of multimodal training, \citeA{mm-shap} proposed MM-Shap, inspired by Shapley values, to evaluate the contribution of each modality.

% In this work, we adopt MM-Shap as an interpretation method to analyze changes in the contribution of each modality.
% MM-Shap is specifically designed to handle multimodal inputs without considering the accuracy of a deep learning model. 
% It provides a quantitative measure of the contribution of each modality.