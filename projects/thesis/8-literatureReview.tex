\chapter{LITERATURE REVIEW}
This section of the literature review is organized around three key topics relevant to our study.
The first topic addresses VL models, providing an overview of the model architectures recently used in VL models and discussing the choice of the base architecture for the VL model used in this research.
The second topic explores probing and interpretability, which are essential for analyzing experimental outcomes.
The final topic focuses on masked language modeling (MLM), an important pre-training approach that has improved VL model performance.
Together, these sections provide a comprehensive overview of the methodological foundations of this study.

\section{Vision-Language model}
Recent advancements in VL model training can be roughly categorized into three main methods. 
The first approach is an individual unimodal model encoder for each modality, such as CLIP \cite{clip} and Align \cite{align}.
This method is trained with the objective to align the intermediate output of each modality encoding. 
The second method utilizes a cross-attention layer to fuse multimodal input, e.g., Flamingo \cite{flamingo}, mPlug \cite{mplug}, LXMERT \cite{lxmert}, and ALBEF \cite{albef}. 
With the cross-attention layer, the model can fuse each modality more deeply. 
Finally, the third approach is a single large attention model with the concatenation of image and text tokens as input, such as BEIT \cite{beit-3}. 
This approach allows each modality to be fused in the early stage, although it requires the highest amount of computational resources.
In this work, we adopt the cross-attention method as the base model due to its ability to fuse each modality input.
Additionally, this approach enables the model to be trained using the Masked Language Modeling (MLM) task.


\section{Probing and interpretability}
In order to analyze deep learning models, it is important to utilize both probing and interpretation methods. 
We can divide model-agnostic methods into two categories: white-box methods, which leverage the internal structure and parameters of deep learning models to interpret the reasoning behind their outputs, and black-box methods, where we modify the input and observe the resulting changes in the model's output.

For white-box methods, Grad-CAM \citeA{grad-cam} is a technique used with CNNs and Vision Transformers. 
This method considers the gradient of the input to interpret the reasoning behind the model’s prediction. 
\citeA{attention-explanations} proposed a method for interpreting attention scores. 
However, these white-box methods are not well-suited for analyzing multimodal deep learning models to understand interactions between different modalities.

For black-box methods, DIME \cite{dime} proposed an interpretation technique that probes a deep learning model using an interpretable linear function over the model’s output. 
Another method to consider is Amnesic Probing \cite{amnesic-probing}, which uses causal intervention by removing parts of the input. 
This allows us to measure the contribution of each part of the input to the model's output. 
In the context of multimodal training, \citeA{mm-shap} proposed MM-Shap, inspired by Shapley values, to evaluate the contribution of each modality.

In this work, we adopt MM-Shap as an interpretation method to analyze changes in the contribution of each modality.
MM-Shap is specifically designed to handle multimodal inputs without considering the accuracy of a deep learning model. 
It provides a quantitative measure of the contribution of each modality.

\section{Masked Language Modelling}
