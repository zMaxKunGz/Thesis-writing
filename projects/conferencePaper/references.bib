%%% Text Encoder model
%BERT
@article{bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

%%% SEMI SUPERVISED LEARNING
% Meta Pseudo Label
@inproceedings{meta_pseudo,
    title={Meta pseudo labels},
    author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={11557--11568},
    year={2021}
}

% VIT Semi Supervised
@inproceedings{semisup-vit,
    title={Semi-supervised Vision Transformers at Scale},
    author={Zhaowei Cai and Avinash Ravichandran and Paolo Favaro and Manchen Wang and Davide Modolo and Rahul Bhotika and Zhuowen Tu and Stefano Soatto},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=7a2IgJ7V4W}
}

@article{aug_consistency,
    title={Unsupervised data augmentation for consistency training},
    author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={6256--6268},
    year={2020}
}

%Temperal Ensembling
@article{laine2016temporal,
    title={Temporal ensembling for semi-supervised learning},
    author={Laine, Samuli and Aila, Timo},
    journal={arXiv preprint arXiv:1610.02242},
    year={2016}
}

%EMA Teacher
@article{mean_teacher,
    title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
    author={Tarvainen, Antti and Valpola, Harri},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

%EMAN Teacher
@inproceedings{eman,
    title={Exponential moving average normalization for self-supervised and semi-supervised learning},
    author={Cai, Zhaowei and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Tu, Zhuowen and Soatto, Stefano},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={194--203},
    year={2021}
}

%FixMatch
@article{fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

%Interpolation consistency training
@article{inter_consistency,
    title={Interpolation consistency training for semi-supervised learning},
    author={Verma, Vikas and Kawaguchi, Kenji and Lamb, Alex and Kannala, Juho and Solin, Arno and Bengio, Yoshua and Lopez-Paz, David},
    journal={Neural Networks},
    volume={145},
    pages={90--106},
    year={2022},
    publisher={Elsevier}
}

%MixUp
@article{mixup,
    title={mixup: Beyond empirical risk minimization},
    author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
    journal={arXiv preprint arXiv:1710.09412},
    year={2017}
}

%MixMatch
@article{mixmatch,
    title={Mixmatch: A holistic approach to semi-supervised learning},
    author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

%Unsupervised Data Augmentation
@inproceedings{cubuk2020randaugment,
    title={Randaugment: Practical automated data augmentation with a reduced search space},
    author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
    pages={702--703},
    year={2020}
}

%%% Self-Distillation & Knowledge distillation
@inproceedings{toward_understanding,
    title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
    author={Zeyuan Allen-Zhu and Yuanzhi Li},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=Uuf2q9TfXGA}
}

%Self-Distillation
@inproceedings{born_again,
    title={Born again neural networks},
    author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
    booktitle={International Conference on Machine Learning},
    pages={1607--1616},
    year={2018},
    organization={PMLR}
}

@inproceedings{noisy_student,
    title={Self-training with noisy student improves imagenet classification},
    author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={10687--10698},
    year={2020}
}

@inproceedings{be_yourown_teacher,
    title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
    author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={3713--3722},
    year={2019}
}

@inproceedings{knowledge_distill,
    author = {Hinton, Geoffrey and Dean, Jeff and Vinyals, Oriol},
    year = {2014},
    month = {03},
    pages = {1-9},
    title = {Distilling the Knowledge in a Neural Network}
}

%Contrastive Knowledge distillation
@inproceedings{tian2020Contrastive,
    title={Contrastive Representation Distillation},
    author={Yonglong Tian and Dilip Krishnan and Phillip Isola},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SkgpBJrtvS}
}

%%% MULTIMODAL MODEL
@inproceedings{lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18123--18133},
  year={2022}
}

% CLIP model
@inproceedings{clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle={International conference on machine learning},
    pages={8748--8763},
    year={2021},
    organization={PMLR}
}

%BLIP
@inproceedings{blip-1,
    title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
    author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
    booktitle={International Conference on Machine Learning},
    pages={12888--12900},
    year={2022},
    organization={PMLR}
}

@article{blip-2,
    title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
    author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
    journal={arXiv preprint arXiv:2301.12597},
    year={2023}
}

%Align Model
@inproceedings{align,
    title={Scaling up visual and vision-language representation learning with noisy text supervision},
    author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
    booktitle={International Conference on Machine Learning},
    pages={4904--4916},
    year={2021},
    organization={PMLR}
}

%CoCa
@article{coca,
    title={CoCa: Contrastive Captioners are Image-Text Foundation Models},
    author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2022},
    url={https://openreview.net/forum?id=Ee277P3AYC},
    note={}
}

%BEIT
@inproceedings{beit-1,
    title={{BE}iT: {BERT} Pre-Training of Image Transformers},
    author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=p-BhZSz59o4}
}

%Flamingo
@article{flamingo,
    title={Flamingo: a visual language model for few-shot learning},
    author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={23716--23736},
    year={2022}
}

@article{albef,
    title={Align before fuse: Vision and language representation learning with momentum distillation},
    author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
    journal={Advances in neural information processing systems},
    volume={34},
    pages={9694--9705},
    year={2021}
}

@article{mplug,
    title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
    author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
    journal={arXiv preprint arXiv:2205.12005},
    year={2022}
}

@inproceedings{pali,
    title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
    author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=mWVoBz4W0u}
}

@inproceedings{beit-3,
    title={Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks},
    author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={19175--19186},
    year={2023}
}

@article{vlmo,
    title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
    author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={32897--32912},
    year={2022}
}

@inproceedings{uniter,
title = {UNITER: UNiversal Image-TExt Representation Learning},
author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
year = {2020},
isbn = {978-3-030-58576-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58577-8_7},
doi = {10.1007/978-3-030-58577-8_7},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX},
pages = {104–120},
numpages = {17},
location = {Glasgow, United Kingdom}
}


%%% VISION MODEL
% Transformer
@inproceedings{vit,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

%ResNet
@inproceedings{resnet,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

%Dino
@inproceedings{dino,
    title={Emerging properties in self-supervised vision transformers},
    author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    pages={9650--9660},
    year={2021}
}

@INPROCEEDINGS{wide_resnet,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

%%% Dataset
% ImageNet
@inproceedings{imagenet,
    title={Imagenet: A large-scale hierarchical image database},
    author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
    booktitle={2009 IEEE conference on computer vision and pattern recognition},
    pages={248--255},
    year={2009},
    organization={Ieee}
}

%Cifar
@article{cifar,
    title={Learning multiple layers of features from tiny images},
    author={Krizhevsky, Alex and Hinton, Geoffrey and others},
    year={2009},
    publisher={Toronto, ON, Canada}
}