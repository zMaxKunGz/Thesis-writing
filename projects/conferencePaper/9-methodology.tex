\chapter{METHODOLOGY}

The proposed self-distillation method training process is described in Figure~\ref{fig:methodology}.
The first step in the training process was to train the image-text representation head by freezing both the image and text encoder model as shown in Figure~\ref{fig:methodology} a).
The second step was self-distillation with combined text and image representation output from the image-text representation head as shown in Figure~\ref{fig:methodology} b).
Difference image and text encoder models pair were chosen to demonstrate the benefit of our method.
We compared our approach with other self-distillation methods \shortcite{furlanello2018born,xie2020self}.
The detail for each part of this experiment is provided in this section.

\begin{figure}[h]
    \caption{Training methodology}
    \label{fig:methodology}
    \begin{center}
        \includegraphics[width=1\textwidth]{Images/Methodology.png}
    \end{center}
    \small a) Training image-text representation head using cross entropy loss b) Self-distillation training by freezing all teacher model
\end{figure}

\begin{figure}[h]
    \caption{Image-Text Cross Attention Classification head}
    \label{fig:cross_attention}
    \centering
    \includegraphics[width=0.5\textwidth]{Images/CrossAttention.png}
\end{figure}

\section{Image-Text Representation Head Training}
% \textcolor{red}{todo:change method image to use sub figure a and b}
In the first step as shown in Figure~\ref{fig:methodology}a), the image-text representation head was trained with image-text pairs $(x_i, t_i)$, where $x_i$ was an $i^{th}$ image input and $t^{th}$ was an $i^{th}$ text created with a prompt ``This is an image of [Class]''.
The teacher image encoder $\theta_{IE}$ and the text encoder $\theta_{TE}$ in the training were pre-trained and frozen.
The image and text encoding were obtained by a mapping function $x'_i = f(x_i; \theta_{IE})$ and $t'_i = f(t_i; \theta_{TE})$ respectively.
The image-text representation head as shown in Figure~\ref{fig:cross_attention} which is based on a cross-attention and linear classification layer, produced logits output as Eq.\ref{eq:cross-attention}.
Then, the logits output from the image-text representation head transformed into probability distribution output with a softmax function.

\begin{equation}
    \label{eq:cross-attention}
    \hat{y}_i = \texttt{Softmax}(\texttt{Attention}(K=x'_i, Q=t'_i, V=x'_i))
\end{equation}

The loss $\mathcal{L}_{\texttt{classification}}$ for training the image-text representation head was a cross-entropy as Eq.\ref{eq:loss-cross-attention}, where $y_i\in\{ 0,1 \}^{C}$ is a one-hot encoded label, $C$ is the number of classes and $N$ is the number of training samples.

\begin{equation}
    \label{eq:loss-cross-attention}
    \mathcal{L}_{\texttt{classification}} = -\sum_{i=1}^{N} y_i\log\hat{y}_i
\end{equation}

\section{Self-Distillation}
After the image-text representation head was trained, the image-text representation head was frozen during the self-distillation process.
For the student model, we created a new image encoder model with the same architecture as the teacher image encoder model, but with different initialized parameters.
A linear classification and softmax layer was added on top of the student image encoder model to produce output distribution $\hat{s}_i$ for the self-distillation process.
The target for training self-distillation was the softmax output $\hat{y}_i$ from the image-text representation head with cross-entropy loss as a loss function.
The loss $\mathcal{L}_{\texttt{distillation}}$ for self-distillation was the cross-entropy loss as shown in Eq \ref{eq:loss_self_dist}.

\begin{equation}
    \label{eq:loss_self_dist}
    \mathcal{L}_{\texttt{distillation}} = -\sum_{i=1}^{N} \hat{y}_i\log\hat{s}_i
\end{equation}

% \subsection{Teacher student}
% For the teacher model, we will use two stream encoder based model same as CLIP model \shortcite{dosovitskiy2021an}.
% In this experiment, the teacher vision encoder model will be ResNet \shortcite{he2016deep} and ViT \shortcite{dosovitskiy2021an} version.
% For the student model we used the same architecture as teacher vision encoder model, which are ResNet and ViT.
% \textcolor{red}{todo: Add table describes both image and text encoders.}

% \section{Training Objectives}
% In the first step, we trained the image-text representation head with benchmark datasets by using Cross Entropy loss as describe in \ref{fig:overall_method} a). The image and text encoder was freezed during the first step training. For text input, we used "This is the image of [Class]" as a prompt \shortcite{radford2021learning}. After the first image-text representation head were trained, we create a new student model which have the same architecture as a image encoder model with a linear classification head. The student model was randomly initialized parameters. The objective for self-distillation with teacher and student is

\section{Evaluation}
In this work, we evaluated the student model with accuracy using an image classification task.
The benchmarks for evaluation were ImageNet, CIFAR-10 and CIFAR-100.
The student model was evaluated compared to the teacher image encoder model using linear probing and student model trained with self-disillation using a single image encoder as a teacher model as shown in Table \ref{tab:experiment_table}.

\begin{table}[h]
    \caption{Experiment evalutation}
    \label{tab:experiment_table}
    \begin{adjustbox}{width=1\textwidth}
        \begin{tabular}{|l|l|l|llll|llll|}
            \hline
            \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Teacher Image\\ Encoder\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Image Encoder \\ Parameters\end{tabular}} & \multirow{2}{*}{Text Encoder} & \multicolumn{4}{l|}{Self-Distillation without Text} & \multicolumn{4}{l|}{Self-Distillation with Text} \\ \cline{4-11} 
             &  &  & \multicolumn{1}{l|}{CIFAR10} & \multicolumn{1}{l|}{CIFAR100} & \multicolumn{1}{l|}{ImageNet Top1\%} & ImageNet Top5\% & \multicolumn{1}{l|}{CIFAR10} & \multicolumn{1}{l|}{CIFAR100} & \multicolumn{1}{l|}{ImageNet Top1\%} & ImageNet Top5\% \\ \hline
            ViT-B/32 & 86M & RoBERTa & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
            ViT-B/32 & 86M & CLIP & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
            ViT-B/16 & 86M & RoBERTa & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
            ViT-B/16 & 86M & CLIP & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
            ResNet-50 & 102M & RoBERTa & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
            ResNet-50 & 102M & CLIP & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \\ \hline
        \end{tabular}
    \end{adjustbox}
\end{table}

\section{Ablation Study}
\subsection{Few-shot learning}
As this method provided texts for training student image encoder models, the texts provided additional information for better image representations.
Consequently, the student model benefited from our method in few-shot learning situations.
In this part, we provided benchmark results for few-shot learning situations.

\subsection{Using Image captioning as a prompt}
For a better understanding of the effect of text prompts in our self-distillation method, we experimented by providing better descriptive prompts.
The image captioning model was used to create image descriptions for the self-distillation process.
Multiple image captioning models were tested for a prompt generation.

\subsection{Repeatation self-distillation}
By using the student as a teacher model for training another student model repeatedly, the performance increased gradually over each generation of the student model \shortcite{furlanello2018born, xie2020self}.
In this work, we also investigated the performance increase over each generation of the student model using our self-distillation method.

\subsection{Image-Text Retrieval}
By increasing performance in the student model using our method with textual information, we suggest that the student would be a good image encoder which also has information about text.
Such that, we can use our method to improve image-text retrieval tasks.
In this ablation study, we provided a result of using a student as an image encoder for the image-text retrieval benchmark, which could test the image encoder's higher semantic understanding and multimodal capability.

% - explainability of the student model like dino or segmentation downstream task. This one aim to test low level semantics information.
% - Repeatation self-distillation like the BornAgain paper.
% - Down stream task with image-text retrieval task.

% \subsection{Error analysis}
% - analysis image task benefit from text which case increase performance and which case not.
% - Few-shot